---
title: "Web scraping"
author: "Illes & Dades"
date: "`r Sys.Date()`"
output: html_document
---

```{r include=FALSE}
START <- Sys.time()
```

# Prerequisits

## Paquets necessaris

```{r message=FALSE, warning=FALSE}
library("rvest")
library("stringr")
library("tidyverse")
library("pxR")

# source("functions_metadades.R") 
source("Functions/llegir_px_csv_i_metadades_fast.R")
directory <- getwd()
```

# Web Scraping a la pagina web de l'IBESTAT

Per començar, cal trobar tots els links de descarrega de les bases de dades. Per fer-ho fet web scraping a la pàgina de l'ibestat manyalment i després s'usa el paquet *rvest*. S'agafa el codi de la pagina, es selecciona tots els links que contenen els fitxer .px i es preparen els links per que es puguin descarregar fàcilment. Això ho he fet al fitxer *web_scraping.R* i he creat un arxiu CSV anomenat links_to_download.

```{r web_scraping}
# fet a 0_web_scraping.Rmd
```

Cream un arxiu csv que conté el links i el nom de la bbdd. Aquest fitxer es diu **links_to_download.csv**

```{r preparar_arxius}
links_to_download <- read.csv("Data/links_to_download.csv")

# canvis de noms de variables i mes:
# links_to_download <- links_descarregar
# directory <- directori
# metadata_### <- doc_metadata_####
# key_var <- var_distintives
# metadata_links_2 <- metadata_links_clean
# metadata_raw <- metadata_util

```

***CODI PER NO HAVER DE COMPILAR PER LES 22280 bbdd sinó per n***

```{r subdataset, eval=FALSE, include=FALSE}
set.seed(142)
links_to_download<-links_to_download[sample(nrow(links_to_download), 30, replace = FALSE),]

nrow(links_to_download)
```

# Maneig BBDD

```{r PACIENCIA, message=FALSE, warning=FALSE}
setwd(paste0(directory, "/Data/"))

n_subset <- 10000
partitions <- ceiling(nrow(links_to_download)/n_subset)

# reading the metadata from multiple DDBB, do it whit partitions cause it's easy to debug
for (i in 1:partitions) {
  
  links_mini <- links_to_download[(i*n_subset-(n_subset-1)):(i*n_subset),]
  
  DADES <- llegir.px.csv.i.metadades.fast(links_mini)
  
  # write(DADES$errors$error_lectura_px, paste0("error_lectura_px_", i, ".txt"))
  # write(DADES$errors$error_lectura_df, paste0("error_lectura_df_", i, ".txt"))
  
  write.csv(DADES$doc_metadata_util, paste0("metadata_raw_", i, ".csv"), fileEncoding = "UTF-8")
  # print(paste0("Feta la partició: ", i))
}

metadata_raw <- tibble(read.csv(paste0("metadata_raw_1.csv"), fileEncoding = "UTF-8"))

if(partitions>1){
  for (i in 2:partitions) {
    metadata_raw <- tibble( 
      merge(
        metadata_raw,
        read.csv(paste0("metadata_raw_", i, ".csv"), fileEncoding = "UTF-8"),
        all = TRUE
      )
    )
  }
}

for (i in 1:partitions){
  file.remove(paste0("metadata_raw_", i, ".csv"))
}

print(paste0("nrow: ", nrow(unique(metadata_raw))))

```

## Seleccionar i millorar columnes per a la recerca

A continuació el que es fa es seleccionar les columnes més important de metadata_raw. I es crea el fitxer **metadata_links.csv** que a més conté els links de descarrega, el que permetra fer el cercador i baixar les dades online.

```{r metadata_links_0}
key_var <- c("CONTENTS", "CONTENTS.ca.", "CREATION.DATE", "DESCRIPTION", "DESCRIPTION.ca.", "INFO", "INFO.ca.", "MATRIX", "REFPERIOD", "REFPERIOD.ca.", "SUBJECT.AREA" , "SUBJECT.AREA.ca.", "SUBJECT.CODE", "SURVEY", "SURVEY.ca.", "TITLE", "TITLE.ca.")

metadata_mini <- metadata_raw[key_var]

links_to_download <- links_to_download[,-1]

names(links_to_download) <- c("Titol_cat", "Enllac", "MATRIX" )

metadata_links <- full_join(links_to_download, metadata_mini, by="MATRIX")

# Es crea una fila duplicada i no se pq, l'elimin
metadata_links <- unique(metadata_links)
```

Tot seguit crea un fitxer que es metadata_links_clean.csv on llev els "\\n" dividesc millor certes columnes per millorar el cercador. i ho guard, com **metadata_links.csv**

```{r metadata_links_noves_col_PACIENCIA_0}
#delate /n & creating easy way to navegate the data

setwd(paste0(directory, "/Data/"))


metadata_links_clean <- metadata_links
for (i in 1:nrow(metadata_links)) {
  for (j in 1:ncol(metadata_links)) {
    metadata_links_clean[i , j] <-  str_remove_all(metadata_links[i , j], '"\n"')
  }
}

metadata_links_clean_parcial <- metadata_links_clean[,"MATRIX"]
metadata_links_clean_parcial <- as.data.frame(metadata_links_clean_parcial)

for (i in 1:nrow(metadata_links_clean)) {
  parte <- str_split(metadata_links_clean[i , "SUBJECT.AREA"], " > ")
  metadata_links_clean_parcial[i, 2] <- parte[[1]][1]
  metadata_links_clean_parcial[i, 3] <- parte[[1]][2]
  
  part <- str_split(metadata_links_clean[i , "SUBJECT.AREA.ca."], " > ")
  metadata_links_clean_parcial[i, 4] <- part[[1]][1]
  metadata_links_clean_parcial[i, 5] <- part[[1]][2]
  
  nombre <- str_split(as.character(metadata_links_clean[i , "SUBJECT.CODE"]), '\\.')
  metadata_links_clean_parcial[i, 6] <- nombre[[1]][1]
  metadata_links_clean_parcial[i, 7] <- nombre[[1]][2]
}


joining_names <- colnames(metadata_links_clean)
joining_names <- joining_names[-c(1, 2, 3)]

metadata_raw <- select(metadata_raw, -joining_names)

names(metadata_links_clean_parcial) <- c("MATRIX", "SUBJECT.AREA.1", "SUBJECT.AREA.2", "SUBJECT.AREA.ca.1", "SUBJECT.AREA.ca.2", "SUBJECT.CODE.1", "SUBJECT.CODE.2")

metadata_links_clean <- full_join(metadata_links_clean, metadata_links_clean_parcial, by="MATRIX")

# creac que aqui hi ha un error, pero ho he de compilar tot per comprovar-ho, CREC que hauria de guardar es fixer metadata_links_clean!
write.csv(metadata_links, "metadata_links.csv", fileEncoding = "UTF-8")


all_metadata_links <- full_join(metadata_raw, metadata_links_clean, by="MATRIX")
write.csv(all_metadata_links, "all_metadata_links.csv", fileEncoding = "UTF-8")
```

# *Falta data (fecha)*

```{r, revisar1}
setwd(paste0(directory, "/Data/"))

col_names <- colnames(all_metadata_links)

### no info
delete <- c("X", "AGGREGALLOWED", "AUTOPEN", "AXIS.VERSION", "CHARSET", "COPYRIGHT",
            "DESCRIPTIONDEFAULT", "CONTVARIABLE", "CONTVARIABLE.ca.", "LANGUAGE", "LANGUAGES")

### trivial info
maybe_delete <- c("CODES", "CODES.ca.", "MAP", "MAP.ca.", "PRECISION", "PRECISION.ca.", "CONTACT", "CONTACT.ca.")
###

col_names <- setdiff(col_names, delete)
col_names <- setdiff(col_names, maybe_delete)


# for(i in 1:length(col_names)){
#   print(paste0("Nom columna :", col_names[i]))
#   unique_data <- unique(dades[col_names[i]])
#   num_data <- nrow(unique_data)
#   print(paste0("Hi ha ", num_data, " dades uniques a la columna"))
#   #print(head(unique(dades[col_names[i]]), 10))
# }


imp_metadata_links <- all_metadata_links[col_names]

write.csv(imp_metadata_links,"imp_metadata_links.csv", fileEncoding = "UTF-8")
```


```{r, revisar2}
setwd(paste0(directory, "/Data/"))

links_to_actualize <- imp_metadata_links[imp_metadata_links$"UPDATE.FREQUENCY" != "No se actualiza" &
                                           !is.na(imp_metadata_links$"UPDATE.FREQUENCY"), ]

links_to_actualize <- links_to_actualize[c("Titol_cat", "Enllac", "Noms_dades")]
names(links_to_actualize) <- c("Titol_cat", "Enllac", "Nom_BBDD")


write.csv(links_to_actualize, "links_to_actualize.csv", fileEncoding = "UTF-8")
```


```{r include=FALSE}
END <- Sys.time()
```

```{r echo=FALSE}
print(END-START)
```
