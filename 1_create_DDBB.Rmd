---
title: "Web scraping"
author: "Illes & Dades"
date: "`r Sys.Date()`"
output: html_document
---

```{r include=FALSE}
START <- Sys.time()
```

# Prerequisits

## Paquets necessaris

```{r message=FALSE, warning=FALSE}
INICI <- Sys.time()
library("rvest")
library("stringr")
library("tidyverse")
library("pxR")

source("functions_metadades.R") # we use: 
# source("Functions/llegir_px_csv_i_metadades_fast.R")
directory <- getwd()

```

# Web Scraping a la pagina web de l'IBESTAT

Per començar, cal trobar tots els links de descarrega de les bases de dades. Per fer-ho fet web scraping a la pàgina de l'ibestat manyalment i després s'usa el paquet *rvest*. S'agafa el codi de la pagina, es selecciona tots els links que contenen els fitxer .px i es preparen els links per que es puguin descarregar fàcilment. Això ho he fet al fitxer *web_scraping.R* i he creat un arxiu CSV anomenat links_to_download.

```{r web_scraping}
# fet a 0_web_scraping.Rmd
```

Cream un arxiu csv que conté el links i el nom de la bbdd. Aquest fitxer es diu **links_to_download.csv**

```{r preparar_arxius}
# links_descarregar <- read.csv("links_descarregar.csv")
getwd()
links_to_download <- read.csv("Data/links_to_download.csv")


# canvis de noms de variables i mes:
# links_to_download <- links_descarregar
# directory <- directori
# metadata_### <- doc_metadata_####

```

***CODI PER NO HAVER DE COMPILAR PER LES 22280 bbdd sinó per n***

```{r subdataset}
set.seed(142)
links_to_download<-links_to_download[sample(nrow(links_to_download), 1500, replace = FALSE),]

nrow(links_to_download)
```


```{r PACIENCIA, message=FALSE, warning=FALSE}

setwd(paste0(directory, "/Data/"))
n_subset <- 1000
partitions <- ceiling(nrow(links_to_download)/n_subset)


inici <- Sys.time()

for (i in 1:partitions) {
  
  links_mini <- links_to_download[(i*n_subset-(n_subset-1)):(i*n_subset),]
  
  DADES <- llegir.px.csv.i.metadades.fast(links_mini)
  
  write(DADES$errors$error_lectura_px, paste0("error_lectura_px_", i, ".txt"))
  write(DADES$errors$error_lectura_df, paste0("error_lectura_df_", i, ".txt"))
  
  write.csv(DADES$metadata_util, paste0("metadata_util_", i, ".csv"), fileEncoding = "UTF-8")
  print(paste0("Feta la partició: ", i, " de ", partitions, "."))
}
fi <- Sys.time()
fi-inici

metadata_util <- tibble()
setwd(paste0(directory, "/Data/"))
getwd()
for (i in 1:partitions) {
 
  metadata_util <- tibble( 
    merge(
      metadata_util,
      read.csv(paste0("metadata_util_", i, ".csv"), fileEncoding = "UTF-8"),
      all = TRUE
    )
  )
}


metadata_util <- DADES$metadata_util

print(paste0("nrow: ", nrow(unique(metadata_util))))

```

## Seleccionar i millorar columnes per a la recerca

A continuació el que es fa es seleccionar les columnes més important de metadata_util. I es crea el fitxer **metadata_links.csv** que a més conté els links de descarrega, el que permetra fer el cercador i baixar les dades online.

```{r metadata_links}
var_distintives <- c("CONTENTS", "CONTENTS.ca.", "CREATION.DATE", "DESCRIPTION", "DESCRIPTION.ca.", "INFO", "INFO.ca.", "MATRIX", "REFPERIOD", "REFPERIOD.ca.", "SUBJECT.AREA" , "SUBJECT.AREA.ca.", "SUBJECT.CODE", "SURVEY", "SURVEY.ca.", "TITLE", "TITLE.ca.")

metadata_mini <- metadata_util[var_distintives]

links_to_download <- links_to_download[,-1]

names(links_to_download) <- c("Titol_cat", "Enllac", "MATRIX" )

metadata_links <- full_join(links_to_download, metadata_mini, by="MATRIX")

# Es crea una fila duplicada i no se pq, l'elimin
metadata_links <- unique(metadata_links)
```

Tot seguit crea un fitxer que es metadata_links_2.csv on llev els "\\n" dividesc millor certes columnes per millorar el cercador. i ho guard, com **metadata_links.csv**

```{r metadata_links_noves_col_PACIENCIA}
#eliminar /n i posar coses guapes

metadata_links_2 <- metadata_links
for (i in 1:nrow(metadata_links)) {
  for (j in 1:ncol(metadata_links)) {
    metadata_links_2[i , j] <-  str_remove_all(metadata_links[i , j], '"\n"')
  }
}

metadata_links_2_parcial <- metadata_links_2[,"MATRIX"]
metadata_links_2_parcial <- as.data.frame(metadata_links_2_parcial)

for (i in 1:nrow(metadata_links_2)) {
  parte <- str_split(metadata_links_2[i , "SUBJECT.AREA"], " > ")
  metadata_links_2_parcial[i, 2] <- parte[[1]][1]
  metadata_links_2_parcial[i, 3] <- parte[[1]][2]
  
  part <- str_split(metadata_links_2[i , "SUBJECT.AREA.ca."], " > ")
  metadata_links_2_parcial[i, 4] <- part[[1]][1]
  metadata_links_2_parcial[i, 5] <- part[[1]][2]
  
  nombre <- str_split(as.character(metadata_links_2[i , "SUBJECT.CODE"]), '\\.')
  metadata_links_2_parcial[i, 6] <- nombre[[1]][1]
  metadata_links_2_parcial[i, 7] <- nombre[[1]][2]
}

names(metadata_links_2_parcial) <- c("MATRIX", "SUBJECT.AREA.1", "SUBJECT.AREA.2", "SUBJECT.AREA.ca.1", "SUBJECT.AREA.ca.2", "SUBJECT.CODE.1", "SUBJECT.CODE.2")

metadata_links_2 <- full_join(metadata_links_2, metadata_links_2_parcial, by="MATRIX")

setwd(paste0(directory, "/Data/"))
write.csv(metadata_links_2, "metadata_links.csv", fileEncoding = "UTF-8")
```

# *Falta data (fecha)*

```{r include=FALSE}
END <- Sys.time()
```

```{r echo=FALSE}
print(END-START)
```
