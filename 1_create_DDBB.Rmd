---
title: "Create BBDD"
author: "Illes & Dades"
date: "`r Sys.Date()`"
output: html_document
---

```{r include=FALSE}
START <- Sys.time()
```

# Prerequisits

## Paquets necessaris

```{r message=FALSE, warning=FALSE}
library("rvest")
library("stringr")
library("tidyverse")
library("pxR")

# source("functions_metadades.R") 
#source("Functions/llegir_px_csv_i_metadades_fast.R")
source("Functions/llegir_px_csv_i_metadades_fast.R")
source("Functions/rm_duplicated_data.R")
source("Functions/reformate_date.R")



directory <- paste0(getwd(), "/Prova")
#directory <- getwd()

```

## Web Scraping a la pagina web de l'IBESTAT

Per començar, cal trobar tots els links de descarrega de les bases de dades. Per fer-ho fet web scraping a la pàgina de l'ibestat manyalment i després s'usa el paquet *rvest*. S'agafa el codi de la pagina, es selecciona tots els links que contenen els fitxer .px i es preparen els links per que es puguin descarregar fàcilment. Això ho he fet al fitxer *web_scraping.R* i he creat un arxiu CSV anomenat links_to_download.

```{r web_scraping}
# fet a 0_web_scraping.Rmd
```

Cream un arxiu csv que conté el links i el nom de la bbdd. Aquest fitxer es diu **links_to_download.csv**

```{r preparar_arxius}
links_to_download <- read.csv("Data/links_to_download.csv", fileEncoding = "UTF-8")

# canvis de noms de variables i mes:
# links_to_download <- links_descarregar
# directory <- directori
# metadata_### <- doc_metadata_####
# key_var <- var_distintives
# metadata_links_2 <- metadata_links_clean
# metadata_raw <- metadata_util
# NEW imp_metadata_links

links_to_download[ ,1] <- paste0("ID_", seq(1:nrow(links_to_download)))

names(links_to_download) <- c("ID", "Titol_cat", "Enllac", "Nom_BBDD")
```

***CODI PER NO HAVER DE COMPILAR PER LES 22280 bbdd sinó per n***

```{r subdataset, eval=TRUE, include=FALSE}
set.seed(1420)
links_to_download <- links_to_download[sample(nrow(links_to_download), 300, replace = FALSE),]
```


```{r n_subdataset}
nrow(links_to_download)
```

# Maneig de la BBDD:

## Obternir metainformació

```{r PACIENCIA, message=FALSE, warning=FALSE}
setwd(paste0(directory, "/Data/"))

n_subset <- 10000
partitions <- ceiling(nrow(links_to_download)/n_subset)

# reading the metadata from multiple DDBB, do it whit partitions cause it's easier to debug
for (i in 1:partitions) {
  
  links_mini <- links_to_download[(i*n_subset-(n_subset-1)):(i*n_subset),]
  
  DADES <- llegir.px.csv.i.metadades.fast(links_mini)
  
  # write(DADES$errors$error_lectura_px, paste0("error_lectura_px_", i, ".txt"))
  # write(DADES$errors$error_lectura_df, paste0("error_lectura_df_", i, ".txt"))
  
  write.csv(DADES$doc_metadata_util, paste0("metadata_raw_", i, ".csv"), fileEncoding = "UTF-8")
  # print(paste0("Feta la partició: ", i))
}

metadata_raw <- tibble(read.csv(paste0("metadata_raw_1.csv"), fileEncoding = "UTF-8"))

if(partitions>1){
  for (i in 2:partitions) {
    metadata_raw <- tibble( 
      merge(
        metadata_raw,
        read.csv(paste0("metadata_raw_", i, ".csv"), fileEncoding = "UTF-8"),
        all = TRUE
      )
    )
  }
}

for (i in 1:partitions){
  file.remove(paste0("metadata_raw_", i, ".csv"))
}

print(paste0("nrow: ", nrow(unique(metadata_raw))))

write.csv(metadata_raw, "metadata_raw.csv", fileEncoding = "UTF-8")
```

Al fitxer "metadata_raw.csv" hi ha la meta info de les basses de dades!

## Natejar BBDD: eliminar NA, fer-ho llegible i millorar la separació per categories.

L'arxiu metadata_links_clean, són les metadades amb el link de descarrega i ja s'han llevat caràcters inutils

```{r}
setwd(paste0(directory, "/Data/"))

names(links_to_download) <- c("ID", "Titol_cat", "Enllac", "Nom_BBDD" )

metadata_links_clean  <- full_join(links_to_download, metadata_raw[,-1], by="ID")

metadata_links_clean <- metadata_links_clean[, !colnames(metadata_links_clean) %in% "Noms_dades"]
metadata_links_clean <- metadata_links_clean[!is.na(metadata_links_clean$MATRIX),]

metadata_links_clean <- metadata_links_clean %>% mutate_all(~gsub('"\n"', '', .))
metadata_links_clean <- metadata_links_clean %>% mutate_all(~gsub('","', ', ', .))

metadata_links_clean_parcial <- metadata_links_clean[,"ID"]
metadata_links_clean_parcial <- as.data.frame(metadata_links_clean_parcial)

for (i in 1:nrow(metadata_links_clean)) {
  parte <- str_split(metadata_links_clean[i , "SUBJECT.AREA"], " > ")
  metadata_links_clean_parcial[i, 2] <- parte[[1]][1]
  metadata_links_clean_parcial[i, 3] <- parte[[1]][2]
  
  part <- str_split(metadata_links_clean[i , "SUBJECT.AREA.ca."], " > ")
  metadata_links_clean_parcial[i, 4] <- part[[1]][1]
  metadata_links_clean_parcial[i, 5] <- part[[1]][2]
  
  nombre <- str_split(as.character(metadata_links_clean[i , "SUBJECT.CODE"]), '\\.')
  metadata_links_clean_parcial[i, 6] <- nombre[[1]][1]
  metadata_links_clean_parcial[i, 7] <- nombre[[1]][2]
}

joining_names <- colnames(metadata_links_clean)
joining_names <- joining_names[-c(1, 2, 3, 4)]

names(metadata_links_clean_parcial) <- c("ID", "SUBJECT.AREA.1", "SUBJECT.AREA.2", "SUBJECT.AREA.ca.1", "SUBJECT.AREA.ca.2", "SUBJECT.CODE.1", "SUBJECT.CODE.2")

metadata_links_clean <- full_join(metadata_links_clean, metadata_links_clean_parcial, by="ID")

all_metadata_links <- metadata_links_clean

all_metadata_links <- rm.duplicated.data(all_metadata_links)

all_metadata_links_date <- reformat.date(all_metadata_links[, c("ID" ,"REFPERIOD.ca.")])

all_metadata_links <- full_join(all_metadata_links, all_metadata_links_date, by = "ID")

write.csv(all_metadata_links, "all_metadata_links.csv", fileEncoding = "UTF-8")


```

```{r}

```

Mirar dif entre all_metadata_links i metadata_links_clean

A metadata_links.csv hi ha un minifitxer amb lo més basic

```{r}
setwd(paste0(directory, "/Data/"))

#metadata_raw_2 <- select(metadata_raw, -joining_names)
#
#all_metadata_links <- full_join(metadata_raw_2, metadata_links_clean, by="ID")
#write.csv(all_metadata_links, "all_metadata_links.csv", fileEncoding = "UTF-8")

key_var <- c("Enllac", "ID", "CONTENTS", "CONTENTS.ca.", "CREATION.DATE", "DESCRIPTION", "DESCRIPTION.ca.", "INFO", "INFO.ca.", "MATRIX", "REFPERIOD", "REFPERIOD.ca.", "SUBJECT.AREA" , "SUBJECT.AREA.ca.", "SUBJECT.CODE", "SURVEY", "SURVEY.ca.", "TITLE", "TITLE.ca.", "REFPERIOD.start.", "REFPERIOD.end.")

write.csv(all_metadata_links[key_var], "metadata_links.csv", fileEncoding = "UTF-8")

```

A imp_metadata_links hi ha la info no trivial!

```{r, revisar1}
setwd(paste0(directory, "/Data/"))

col_names <- colnames(all_metadata_links)

### no info
delete <- c("X", "AGGREGALLOWED", "AUTOPEN", "AXIS.VERSION", "CHARSET", "COPYRIGHT",
            "DESCRIPTIONDEFAULT", "CONTVARIABLE", "CONTVARIABLE.ca.", "LANGUAGE", "LANGUAGES")

### trivial info
maybe_delete <- c("CODES", "CODES.ca.", "MAP", "MAP.ca.", "PRECISION", "PRECISION.ca.", "CONTACT", "CONTACT.ca.")
###

col_names <- setdiff(col_names, delete)
col_names <- setdiff(col_names, maybe_delete)


# for(i in 1:length(col_names)){
#   print(paste0("Nom columna :", col_names[i]))
#   unique_data <- unique(dades[col_names[i]])
#   num_data <- nrow(unique_data)
#   print(paste0("Hi ha ", num_data, " dades uniques a la columna"))
#   #print(head(unique(dades[col_names[i]]), 10))
# }


imp_metadata_links <- all_metadata_links[col_names]

write.csv(imp_metadata_links,"imp_metadata_links.csv", fileEncoding = "UTF-8")
```

links_to_actualize Hi ha els links per actualitzar

```{r, revisar2}
setwd(paste0(directory, "/Data/"))

links_to_actualize <- imp_metadata_links[imp_metadata_links$"UPDATE.FREQUENCY" != "No se actualiza" &
                                           !is.na(imp_metadata_links$"UPDATE.FREQUENCY"), ]

links_to_actualize <- links_to_actualize[c("ID", "Titol_cat", "Enllac", "Nom_BBDD")] # eliminat noms_dades?
#names(links_to_actualize) <- c("ID","Titol_cat", "Enllac", "Nom_BBDD")


write.csv(links_to_actualize, "links_to_actualize.csv", fileEncoding = "UTF-8")
```

# *Falta data (fecha)*

```{r include=FALSE}
END <- Sys.time()
```

```{r echo=FALSE}
print(END-START)
```
